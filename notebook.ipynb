{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa4e984",
   "metadata": {},
   "source": [
    "### Explainable AI in Employee Attrition Prediction\n",
    "\n",
    "- [View Solution Notebook](./solutions.html)\n",
    "- [View Project Page](https://www.codecademy.com/content-items/707237fd8b054ff6b5873d92d61c361c)\n",
    "\n",
    "  \n",
    "Predicting employee attrition is challenging for HR departments, as it helps organizations retain valuable talent and reduce turnover costs. However, AI models used in such decisions must be transparent and interpretable, ensuring that HR professionals can trust and understand the predictions.\n",
    "\n",
    "In this project, we will use the IBM HR Analytics Employee Attrition Dataset (https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset) to predict whether an employee will likely leave the company. We will train different machine learning models and apply Explainable AI (XAI) techniques to interpret their predictions.\n",
    "\n",
    "In this project, you will:\n",
    "* Understand how different models predict attrition and which factors matter most.\n",
    "* Use XAI techniques to interpret different models, including model-specific and model-agnostic methods\n",
    "* Utilize visualization tools, like PDP and ICE plots, to show how features influence predictions.\n",
    "\n",
    "By the end of this project, we will have a fully interpretable pipeline that can help HR professionals make informed, fair, and data-driven decisions regarding employee retention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bf7ac-b531-4244-ab30-f201552cc7e3",
   "metadata": {},
   "source": [
    "**Setup - Import Libraries**\n",
    "\n",
    "Run the cell below to import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3021210d-cf83-474b-9f48-e3e3ce395a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723704df-8e96-4e3a-b1e8-a12d26717c9f",
   "metadata": {},
   "source": [
    "## Task Group 1 - Load and Explore the Dataset\n",
    "\n",
    "Let's first import the dataset and explore its structure, data types, missing values, and target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf6eac4-cbac-4c3b-992b-8b05f21dd577",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Load and prepare the attrition dataset:\n",
    "* Load the dataset using pandas and the filename `attrition_data.csv.`\n",
    "* Explore the dataset’s structure, including column names, data types, and the first few rows. Print the data frame head and info.\n",
    "* Check for missing values to determine if any data cleaning is needed.\n",
    "* Examine the distribution of the target variable (Attrition) to understand class balance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc6c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa5b079",
   "metadata": {},
   "source": [
    "This shows that only ~16% of employees left, meaning the dataset is fairly imbalanced.  No missing values are found, so you do not have to handle these.  However, there are categorical variables that will need encoding, and the numerical features will require scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29b0ea",
   "metadata": {},
   "source": [
    "## Task Group 2 - Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4891f3-efab-49fa-9629-a5258e84f037",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Convert the target variable to numerical format (Yes/No → 1/0)\n",
    "- Machine learning models require numerical inputs, so categorical values in the target variable must be mapped to numbers.\n",
    "- The Yes/No values in the \"Attrition\" column should be replaced with 1 (Yes - employee left) and 0 (No - employee stayed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428e6783-5c28-4f02-bdbf-f63512653a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable (Attrition: Yes/No → 1/0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e1604-29a2-4ead-bd91-4e360e97e6c7",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Drop Non-Informative Columns\n",
    "* Some columns do not contribute to prediction and should be removed to reduce noise and improve model performance.\n",
    "* `EmployeeNumber` (Unique ID, irrelevant for predictions)\n",
    "* `Over18` (All values are the same, no variability)\n",
    "* `EmployeeCount` and `StandardHours` (Constant values, provide no useful information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80772970-985b-4f95-b8ce-11bc450a4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-informative columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffacf39-bc2c-410a-acad-04c277f6230e",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "Convert Categorical Features into Numerical (pd.get_dummies)\n",
    "* Many features in the dataset are categorical (e.g., `JobRole,` `BusinessTravel`).\n",
    "* We need to convert them into a numerical format so that machine learning models can process them.\n",
    "* One-hot encoding (`pd.get_dummies`) creates binary columns for each category.\n",
    "* `drop_first=True` prevents redundant dummy variables (avoiding multicollinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d585e62e-09e1-4007-aa95-459c03cfd0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d54e04-c8af-4d7c-8df3-730f21e37790",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "Split the Data into Training and Test Sets\n",
    "* Define `X` and `y` as your features and target.\n",
    "* Split the data using `train_test_split` with `test_size=0.2.`\n",
    "* Use `stratify=y` to ensure that both train and test sets maintain the original class distribution (important when dealing with imbalanced data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4f338d-1998-45ff-9246-9088ba794b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train-test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2009f-942b-4925-93b2-32f99abc5084",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Scale Numerical Features\n",
    "* Some models (Logistic Regression, SVM, Neural Networks) perform better when numerical features are scaled.\n",
    "* Standardization (`StandardScaler`) transforms numerical values to zero mean and unit variance.\n",
    "* Fit `StandardScaler()` on `X_train` and transform `X_test` separately to prevent data leakage.\n",
    "* Saved the rescaled features as `X_tran_scaled` and `X_test_scaled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28845dad-b7ef-4500-8216-023834dbce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81158652-96b8-4bda-9ad2-a457dc8251e8",
   "metadata": {},
   "source": [
    "## Task Group 3 - Logistic Regression and Coefficients Interpretation\n",
    "Now, you are ready to train your first model, logistic regression, and use the model's coefficients to explain the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386d09d-927c-4e3d-9aca-85e1497b6e42",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "Train a Logistic Regression Model \n",
    "* Define a logistic regression model using `max_iter=500` to ensure that the model has enough iterations to converge.\n",
    "* Train the model using `.fit()` with `X_train_scaled` and `y_train.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492f2dc8-f0da-4ca1-95fa-a5ee6778b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ad631-79e7-4f38-b1d7-4feb86eaeeb2",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "Extract Model Coefficients\n",
    "* Extract the model coefficients and store them in a Pandas Series with index the features name\n",
    "* Sort the values in the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845c45a7-4020-4b71-a50b-59a719b987ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance (coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f42de-3b86-4d94-979b-17fb49dd3244",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "Plot Feature Importance using a Bar Chart \n",
    "* Bar plots can help visualize which features strongly influence employee attrition.\n",
    "* Features with high positive values contribute to higher attrition rates.\n",
    "* Features with high negative values contribute to lower attrition rates (employees staying).\n",
    "* Use `plt.barh()` and pass in the coefficients indices and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5d613ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature coefficients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bec4a",
   "metadata": {},
   "source": [
    "After running this code, you should see a horizontal bar chart with feature names on the y-axis and their corresponding coefficient values on the x-axis.\n",
    "* Features with high positive values (e.g., `OverTime`) increase the risk of attrition.\n",
    "* Features with high negative values (e.g., `EducationField_Other`, `TotalWorkingYears`) decrease the risk of attrition (they encourage employees to stay)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de97d4-3fa2-4c96-befd-ddfe619e21a1",
   "metadata": {},
   "source": [
    "## Task Group 4 - Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16031a62-7478-447b-92da-523cb39dd6a6",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "Compute Permutation Importance \n",
    "* Calculate the permutation importance for the logistic regression model.  Pass in the model, `X_test_scaled,` and `y_test.`\n",
    "* Use `n_repeats=10` → Runs the permutation process 10 times for more stable results.\n",
    "* Use `random_state=42` → Ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ce8109-d32f-4ce9-b238-b9ef4a2494b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63289a-f4f8-4fb7-82b9-70c24b19159c",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "Convert Results to a Pandas DataFrame\n",
    "* The results from permutation_importance are stored in a dictionary\n",
    "* Extract the mean importance score for each feature and store it in a DataFrame for better visualization and sorting.\n",
    "* Sorting in descending order ensures that the most important features appear at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69600e7a-d9f8-4ef7-b0a5-97bf896eff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfd4c05-1e41-420d-8859-0de0d30a7c2f",
   "metadata": {},
   "source": [
    "### Task 12\n",
    "Plot Permutation Importance as a Bar Chart\n",
    "* Plot the permutation importance as a horizontal bar chart with feature names and permutation importance values as the heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7bf60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8a4de",
   "metadata": {},
   "source": [
    "You should see a horizontal bar chart with feature names on the y-axis and their importance scores on the x-axis. The higher the score, the more \n",
    "\n",
    "* High importance: Features like \"OverTime_Yes\" or \"JobSatisfaction\" may appear at the top, meaning shuffling them hurts model accuracy the most.\n",
    "* Low importance: Features with small or near-zero scores do not contribute significantly to the predictions and might be redundant.\n",
    "\n",
    "What If Some Permutation Importance Values Are Negative?\n",
    "* Negative values in permutation importance can occur, and they indicate that shuffling a particular feature actually improved the model's performance instead of degrading it. This could mean the feature isn't relevant or could correlated with other features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71357f6d-b15e-4c81-bdb5-c626ee9eb2cb",
   "metadata": {},
   "source": [
    "## Task Group 5 - Random Forest Classifier and Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4955a0-d63d-40d6-b4f6-45b0e678cd91",
   "metadata": {},
   "source": [
    "### Task 13\n",
    "Train a Random Forest Classifier\n",
    "* Train a random forest classifier with `n_estimators=100` and `random_state=42.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66c8a3c-7cc6-4f69-b121-ea5ff6e56896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a63ab-01ae-4317-8ea7-ac2c34b34d36",
   "metadata": {},
   "source": [
    "### Task 14\n",
    "Extract Feature Importance \n",
    "* Extract the feature importances from the random forest model\n",
    "* Store the feature importance values in a pandas Series and sort them in ascending order for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d63b00fa-73f8-4923-a2c4-8771c3ac9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76993a-01fe-48dc-9e6a-bee433fc6d1e",
   "metadata": {},
   "source": [
    "### Task 15\n",
    "Plot Feature Importance Using a Bar Chart\n",
    "* Plot the feature importance as a horizontal bar chart with feature names and importance values as the heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78fe7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3dbc6",
   "metadata": {},
   "source": [
    "The features at the top of the chart have the highest importance, meaning they play a crucial role in predicting attrition.  How many of these were top of the logistic regression model coefficents?  Each model is different, so the importance will not match exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a7854-fdaf-46de-b067-8262bb7aa28e",
   "metadata": {},
   "source": [
    "## Task Group 6 - Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffca538-40ff-4dcd-a8b8-c7fda49191a3",
   "metadata": {},
   "source": [
    "### Task 16\n",
    "Select Top 3 Features from the Random Forest model\n",
    "* Select the top three features based on the random forest model feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63966281-4d4e-42e2-a1fc-019fd787292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 3 most important features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540562c-8257-4f9b-9853-921352936b89",
   "metadata": {},
   "source": [
    "### Task 17\n",
    "Generate Partial Dependence Plots (PDP)\n",
    "* Create PDP plots for the top three features using the random forest model and `X_train_scaled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca163dd8-8354-487f-a5c2-34b4e06f936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PDP plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc7ae2-586d-4d31-b8f5-28395aa98d0d",
   "metadata": {},
   "source": [
    "The PDPs show some interesting trends -- the y-axis here is in probability, as the model is a classifier. For `MonthlyIncome,` low values are associated with a higher likelihood of attrition. `Age` has a bimodal effect -- low values have the highest likelihood of attrition, drop for middle ages, and then increase for higher ages. You can also see this with `TotalWorkingYears.` Both young workers and older workers are more likely to attrite; younger workers are probably looking for another opportunity while older workers may be retiring.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efe92e-d30d-46b4-8c5e-0a23fd1bf3cb",
   "metadata": {},
   "source": [
    "### Task 18\n",
    "Generate Combined PDP/ICE Plots for the Top 3 Features\n",
    "* Create Combined ICE/PDP plots for the top three features using the random forest model and `X_train_scaled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9c718af-38cd-4dbc-854f-ed892bae506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create combined PDP/ICE plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69fa1e8-2918-4b26-8667-3cbbed3a2155",
   "metadata": {},
   "source": [
    "The combined ICE/PDP plots highlight the variation in the dataset. The y-axis shows quite a range of values.  \n",
    "The thin blue lines show how each individual employee's prediction changes as the feature is varied, and the thick, dotted, orange line represents the average effect. Since the blue lines are not parallel and vary, the features affect different employees differently (possible interactions with other features).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572af28d-3511-485f-8579-98f6645f0399",
   "metadata": {},
   "source": [
    "### Task 19\n",
    "Generate a 2D Partial Dependence Plot\n",
    "* Create 2D PDP plot using the top _two_features from the random forest feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81d163fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create2D PDP plot for interaction between two features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f799b4c0",
   "metadata": {},
   "source": [
    "The 2D PDP plot generates a color-coded contour plot that shows how attrition probability changes when both `Age` and `MonthlyIncome` are varied together. 2D PDP plots reveal interactions between two features that a 1D plot might miss. For example, you saw that `Age` had a bimodal PDP plot—low and high age values had the highest attrition probabilities. The 2D plot shows that the areas of highest attrition (labeled 0.24 and light green) correspond to low ages but are split between low and high monthly incomes. So young people with middle incomes are less likely to leave than young people with high or low salary ranges. For very high ages, low monthly incomes are associated with a higher attrition likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddd6db",
   "metadata": {},
   "source": [
    "Author: Thato Seluku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb320e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
